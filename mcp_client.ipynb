{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0949e9b9-0b7f-4438-a3a7-99fff07d560a",
   "metadata": {},
   "source": [
    "Implementation of a __LangGraph__ workflow with these nodes:\n",
    "\n",
    "- InputNode $\\rightarrow$ receives player input.\n",
    "- CheckContextNode $\\rightarrow$ gets context from MCP.\n",
    "- ToolDecisionNode $\\rightarrow$ checks if input contains factual query (e.g., keywords: 'what is', 'who is', 'types of', etc.)\n",
    "- ToolCallNode $\\rightarrow$ performs TOOL_CALL via MCP if needed.\n",
    "- PromptAssemblyNode $\\rightarrow$ assembles prompt (Adam's persona + context + player message).\n",
    "- LLMNode $\\rightarrow$ gets GPT response.\n",
    "- OutputNode $\\rightarrow$ logs final message and updates MCP with new dialogue turn.\n",
    "\n",
    "__using LangGraph and OpenAI__ \n",
    "\n",
    "### Code workflow\n",
    "This code sets up a FastAPI server that:\n",
    "- Stores a conversation with the player and Adam.\n",
    "- Manages memory so it fits within a token limit.\n",
    "- Can summarize old conversations.\n",
    "- Can reset the conversation.\n",
    "- Can make tool calls (Wikipedia search) if needed.\n",
    "\n",
    "This code is LangGraph's dialogue engine that powers the NPC \"Adam\"!\n",
    "1. It simulates a conversation loop where:\n",
    "2. It takes player input.\n",
    "3. It fetches past context from the FastAPI server.\n",
    "4. It decides if a Wikipedia tool call is needed.\n",
    "5. It assembles a prompt including persona + context + tool results (if any).\n",
    "6. It sends the final prompt to GPT (using ChatOpenAI) to generate Adam's reply.\n",
    "7. It prints _Adam's_ reply.\n",
    "\n",
    "\n",
    "## Breaking It Down\n",
    "\n",
    "| Component | What it does |\n",
    "|:---|:---|\n",
    "| `persona` | A fixed description of Adam's character. |\n",
    "| `ChatOpenAI` | Calls GPT-3.5-Turbo to generate Adam's responses. |\n",
    "| `StateGraph` | A directed graph where each node is a step in the conversation logic. |\n",
    "| `Nodes` | Functions like `input_node`, `check_context_node`, etc. Each represents a step. |\n",
    "| `ToolNode` (optional) | (Unused here) - could have been used if you had tools as objects. |\n",
    "| `requests` | Used to communicate with your FastAPI memory and tool call server (MCP). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618ed25-85d5-476b-aaeb-c650513ba47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph import ToolNode\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28947f-8a84-48b5-9bdb-81c00f32364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Adam's persona\n",
    "persona = \"Adam is a wise, centuries-old sage from the northern isles. He speaks with empathy and shares ancient lore.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2fcf3-d4a5-43eb-bf25-0b2824cd9dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM (GPT-3.5-Turbo)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49f070-0d5e-409b-97ec-2dde18d44db2",
   "metadata": {},
   "source": [
    "# Node Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f3c08-dda0-4bc7-9873-a06c4298043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Input Node: Get player input\n",
    "def input_node(state):\n",
    "    state[\"user_input\"] = input(\"You: \")  # Get user input\n",
    "    return state, \"check_context\"         # Move to check_context node\n",
    "\n",
    "# 2. Context Fetch Node: Fetch conversation memory\n",
    "def check_context_node(state):\n",
    "    res = requests.get(\"http://localhost:8000/get_context\")  # Call MCP server\n",
    "    state[\"context\"] = res.json()[\"context\"]                 # Save conversation context\n",
    "    return state, \"tool_decision\"                            # Move to tool_decision node\n",
    "\n",
    "# 3. Tool Decision Node: Should we call Wikipedia?\n",
    "def tool_decision_node(state):\n",
    "    question = state[\"user_input\"].lower()\n",
    "    # Check if the input contains factual keywords\n",
    "    if any(kw in question for kw in [\"what is\", \"who is\", \"types of\", \"genres\", \"explain\"]):\n",
    "        return state, \"tool_call\"  # If factual, call tool\n",
    "    return state, \"assemble_prompt\"  # Otherwise, move to assemble_prompt\n",
    "\n",
    "# 4. Tool Call Node: Call Wikipedia if needed\n",
    "def tool_call_node(state):\n",
    "    query = state[\"user_input\"]\n",
    "    res = requests.get(f\"http://localhost:8000/tool_call\", params={\"query\": query})  # Wikipedia API through MCP\n",
    "    state[\"tool_result\"] = res.json()[\"result\"]  # Save Wikipedia result\n",
    "    return state, \"assemble_prompt\"\n",
    "\n",
    "# 5. Assemble Prompt Node: Build final prompt for GPT\n",
    "def assemble_prompt_node(state):\n",
    "    template = PromptTemplate.from_template(\n",
    "        \"{persona}\\n\\nConversation so far:\\n{history}\\n\\nPlayer: {user_input}\\nAdam:\"\n",
    "    )\n",
    "\n",
    "    # Format conversation history into a string\n",
    "    history = \"\\n\".join([f'{m[\"role\"]}: {m[\"content\"]}' for m in state[\"context\"]])\n",
    "\n",
    "    # If we have a tool result, add it to history\n",
    "    if \"tool_result\" in state:\n",
    "        history += f\"\\nTool: {state['tool_result']}\"\n",
    "\n",
    "    # Fill the prompt template\n",
    "    prompt = template.format(persona=persona, history=history, user_input=state[\"user_input\"])\n",
    "    \n",
    "    state[\"final_prompt\"] = prompt\n",
    "    return state, \"llm\"\n",
    "\n",
    "# 6. LLM Node: Generate Adam's reply\n",
    "def llm_node(state):\n",
    "    response = llm.invoke(state[\"final_prompt\"])  # Call GPT\n",
    "    state[\"llm_output\"] = response.content        # Save GPT's output\n",
    "    return state, \"output\"\n",
    "\n",
    "# 7. Output Node: Print Adam's reply and save to memory\n",
    "def output_node(state):\n",
    "    print(f\"Adam: {state['llm_output']}\")  # Show GPT's reply\n",
    "\n",
    "    # Save the conversation back into memory via MCP\n",
    "    requests.post(\"http://localhost:8000/add_message\", json={\"role\": \"user\", \"content\": state[\"user_input\"]})\n",
    "    requests.post(\"http://localhost:8000/add_message\", json={\"role\": \"assistant\", \"content\": state[\"llm_output\"]})\n",
    "    \n",
    "    return state, END  # End the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283a4c4-05eb-4f3d-92c5-5983adb2876b",
   "metadata": {},
   "source": [
    "# Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e060a4c-55b4-4047-b715-f0d4463988e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph()\n",
    "\n",
    "# Add all nodes to the graph\n",
    "workflow.add_node(\"input\", input_node)\n",
    "workflow.add_node(\"check_context\", check_context_node)\n",
    "workflow.add_node(\"tool_decision\", tool_decision_node)\n",
    "workflow.add_node(\"tool_call\", tool_call_node)\n",
    "workflow.add_node(\"assemble_prompt\", assemble_prompt_node)\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"output\", output_node)\n",
    "\n",
    "# Set the starting point\n",
    "workflow.set_entry_point(\"input\")\n",
    "\n",
    "# Define edges (state transitions)\n",
    "workflow.add_edge(\"input\", \"check_context\")\n",
    "workflow.add_edge(\"check_context\", \"tool_decision\")\n",
    "workflow.add_edge(\"tool_decision\", \"tool_call\")\n",
    "workflow.add_edge(\"tool_decision\", \"assemble_prompt\")\n",
    "workflow.add_edge(\"tool_call\", \"assemble_prompt\")\n",
    "workflow.add_edge(\"assemble_prompt\", \"llm\")\n",
    "workflow.add_edge(\"llm\", \"output\")\n",
    "\n",
    "# Compile the graph into an executable application\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2fbd7-103d-4356-bf7f-cddc00607121",
   "metadata": {},
   "source": [
    "# Run the Dialogue Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90329171-2d98-4679-a32e-37e2b036c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        app.invoke({})  # Start the conversation loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
